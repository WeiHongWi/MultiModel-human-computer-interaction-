# MultiModel-human-computer-interaction-
## The purpose of my project is implementation of the paper
"Multimodal Chain: Cross-modal:Collaboration through Listening,Speaking,and Visualizing"
JOHANES EFFENDI1,2, ANDROS TJANDRA∗1
(Non-member, IEEE), SAKRIANI SAKTI1,2
(Member, IEEE), and SATOSHI NAKAMURA1,2, (Fellow, IEEE)


## Introduction
Human interacted with environment with multimodals that processes their senses.For example,the dialogue between speaker and listener,when the messenge sent from speaker and the speech also sent back  the speech to speaker so that he can realize the quality of his speech and may adjust some properties.So,by this idea,the research team previously presented the speech chain model, just like the figure below:

<div align=center><img src="https://github.com/WeiHongWi/MultiModel-human-computer-interaction-/blob/main/Speech%20Machine%20Chain.png" width="400px" height="300px" />
  
<div align=left>
Furthermore,they thought the visual system will help the TTS and ASR to perform better.So,the number of chain became 3.just like the figure below:

<div align=center><img src="https://github.com/WeiHongWi/MultiModel-human-computer-interaction-/blob/main/Multimodal%20and%20single%20loop%20Multimodal.png" width = "500px" height ="250px">

  
  
So,in this project, I will intend to create the single loop multimodal and the simple implementation detail in ASR、TTS、IR、IC model.
